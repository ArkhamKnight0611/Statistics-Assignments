{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of Variance (ANOVA) is a statistical test used to compare means among two or more groups. However, ANOVA relies on several assumptions, and violations of these assumptions can impact the validity of the results. Here are the key assumptions for ANOVA along with examples of violations:\n",
        "\n",
        "Assumption 1: Independence of Observations\n",
        "\n",
        "Assumption: Observations in each group are independent of each other. This means that the values in one group should not be influenced by or dependent on the values in any other group.\n",
        "Violation Example: In a medical study comparing the effectiveness of two drugs, if some patients receive both drugs, the observations are not independent. This could lead to artificially low variability and impact the validity of ANOVA results.\n",
        "Assumption 2: Normally Distributed Data\n",
        "\n",
        "Assumption: The data within each group follows a normal distribution. Normality is important because ANOVA assumes that the sampling distribution of the means is normal, even if the original data is not.\n",
        "Violation Example: In a study measuring the test scores of students in different schools, if the test scores are heavily skewed or do not follow a normal distribution within each school, ANOVA results may be invalid.\n",
        "Assumption 3: Homogeneity of Variance (Homoscedasticity)\n",
        "\n",
        "Assumption: The variances of the groups being compared are roughly equal. In other words, the spread of data points should be approximately the same across all groups.\n",
        "Violation Example: In an experiment comparing the strength of three different types of materials, if one material shows much larger variability in strength compared to the others, violating homogeneity of variance, ANOVA results may not be valid.\n",
        "Assumption 4: Independence of Group Means\n",
        "\n",
        "Assumption: The means of the groups being compared are independent. This means that the groups should not be interrelated or influenced by each other.\n",
        "Violation Example: In a study examining the impact of different teaching methods in schools, if teachers use multiple methods simultaneously, the means of the groups (e.g., classrooms) may not be independent, violating this assumption.\n",
        "Assumption 5: Equal Sample Sizes (for one-way ANOVA)\n",
        "\n",
        "Assumption: For a one-way ANOVA (comparing multiple groups), it is often assumed that the sample sizes are equal across all groups.\n",
        "Violation Example: In a study comparing the performance of employees from different departments, if one department has significantly fewer employees than others, it may violate the assumption of equal sample sizes.\n",
        "Assumption 6: Absence of Outliers\n",
        "\n",
        "Assumption: The data does not contain extreme outliers that can disproportionately affect the results.\n",
        "Violation Example: In a study comparing the salaries of employees in different job roles, if there is a single employee with an extremely high salary compared to others, it can distort the group means and impact the ANOVA results.\n",
        "Violations of these assumptions can lead to Type I errors (false positives) or Type II errors (false negatives) in hypothesis tests. In some cases, data transformation or the use of non-parametric tests may be considered when assumptions are seriously violated. However, it's important to be aware of these assumptions and address them appropriately to ensure the validity of ANOVA results."
      ],
      "metadata": {
        "id": "AyeM4tFCL3nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of Variance (ANOVA) is a statistical technique used to compare means among two or more groups or categories. There are three main types of ANOVA, each designed for specific situations:\n",
        "\n",
        "One-Way ANOVA (One-Factor ANOVA):\n",
        "\n",
        "Situation: One-Way ANOVA is used when you want to compare the means of three or more independent groups or categories to determine if there are statistically significant differences among them. It is used when you have one categorical independent variable (factor) and one continuous dependent variable.\n",
        "Example: You want to compare the average test scores of students who attended three different schools to determine if there is a statistically significant difference in performance between the schools.\n",
        "Two-Way ANOVA (Two-Factor ANOVA):\n",
        "\n",
        "Situation: Two-Way ANOVA is used when you have two categorical independent variables (factors) and one continuous dependent variable. It allows you to simultaneously assess the effects of two factors and their interaction on the dependent variable.\n",
        "Example: You want to study the effects of both gender and age on the performance of athletes. You have two factors: gender (male or female) and age group (young, middle-aged, or senior).\n",
        "Repeated Measures ANOVA:\n",
        "\n",
        "Situation: Repeated Measures ANOVA is used when you have a single group of subjects that you measure repeatedly under different conditions or at different time points. It assesses the impact of a within-subjects factor (the repeated measures factor) on the dependent variable.\n",
        "Example: You are testing the effects of a new drug on blood pressure in a group of patients. You measure their blood pressure before administering the drug, immediately after, and at 1-hour intervals for the next four hours. Here, the factor is \"time\" (pre-drug, post-drug, and different time intervals), and you are assessing how this factor affects blood pressure.\n",
        "In summary, the choice of which type of ANOVA to use depends on the number of categorical independent variables you have and the nature of your research design.\n",
        "\n",
        "Use One-Way ANOVA when you have one categorical independent variable.\n",
        "Use Two-Way ANOVA when you have two categorical independent variables and want to examine their individual effects and interaction.\n",
        "Use Repeated Measures ANOVA when you have a single group of subjects measured under multiple conditions or time points and want to assess the within-subjects effects.\n",
        "Selecting the appropriate ANOVA method ensures that you can analyze your data correctly and draw valid conclusions based on your research design and objectives."
      ],
      "metadata": {
        "id": "aBXumBaJMZX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The partitioning of variance in Analysis of Variance (ANOVA) is a fundamental concept that helps us understand how the total variance in a dataset can be broken down into different components, which are then used to assess the significance of the factors being studied. This concept is crucial in ANOVA for several reasons:\n",
        "\n",
        "Understanding Variability: ANOVA is primarily used to determine whether there are statistically significant differences between groups or conditions. By partitioning the variance, we gain insights into the sources of variability in the data. This helps us understand why some groups or conditions differ from others.\n",
        "\n",
        "Hypothesis Testing: ANOVA involves testing null and alternative hypotheses about the means of different groups. The partitioning of variance provides a clear way to calculate the test statistic (F-statistic) and determine whether the observed differences between groups are larger than what would be expected due to random variability.\n",
        "\n",
        "Interpretation of Results: When ANOVA detects a significant difference, knowing how the variance is partitioned allows us to interpret the results. We can identify which factors or conditions are contributing the most to the observed differences, making it easier to draw meaningful conclusions from the analysis.\n",
        "\n",
        "The partitioning of variance in ANOVA typically involves three main components:\n",
        "\n",
        "Total Variance (Total Sum of Squares, SST): This represents the total variability in the data, without regard to group membership. It is calculated as the sum of squared differences between each data point and the overall mean.\n",
        "\n",
        "Between-Group Variance (Between-Group Sum of Squares, SSB): This component represents the variability between the group means. It is calculated as the sum of squared differences between each group mean and the overall mean, weighted by the number of observations in each group.\n",
        "\n",
        "Within-Group Variance (Within-Group Sum of Squares, SSW): This component represents the variability within each group. It is calculated as the sum of squared differences between individual data points and their respective group means.\n",
        "\n",
        "The partitioning of variance leads to the decomposition of the total variance into two components: the variance explained by the factors (Between-Group Variance) and the unexplained or error variance (Within-Group Variance). The ratio of the explained variance to the unexplained variance (SSB/SSW) is used to calculate the F-statistic, which is then compared to a critical value to determine the statistical significance of the group differences."
      ],
      "metadata": {
        "id": "HgYPXhxlMjk-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_jc2EJVLsrP",
        "outputId": "14b07337-5a9c-4663-b703-30f49edfe192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sum of Squares (SST): 9750.0\n",
            "Explained Sum of Squares (SSE): 9000.0\n",
            "Residual Sum of Squares (SSR): 750.0\n",
            "F-statistic: 72.0\n",
            "P-value: 2.0717621103300345e-07\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Example data for three groups (replace this with your actual data)\n",
        "group1 = [25, 30, 35, 40, 45]\n",
        "group2 = [55, 60, 65, 70, 75]\n",
        "group3 = [85, 90, 95, 100, 105]\n",
        "\n",
        "# Combine all data into one array\n",
        "data = np.concatenate([group1, group2, group3])\n",
        "\n",
        "# Calculate the overall mean (Grand Mean)\n",
        "grand_mean = np.mean(data)\n",
        "\n",
        "# Calculate the Total Sum of Squares (SST)\n",
        "sst = np.sum((data - grand_mean) ** 2)\n",
        "\n",
        "# Calculate the group means\n",
        "mean_group1 = np.mean(group1)\n",
        "mean_group2 = np.mean(group2)\n",
        "mean_group3 = np.mean(group3)\n",
        "\n",
        "# Calculate the Explained Sum of Squares (SSE)\n",
        "sse = len(group1) * (mean_group1 - grand_mean) ** 2  + len(group2) * (mean_group2 - grand_mean) ** 2  + len(group3) * (mean_group3 - grand_mean) ** 2\n",
        "\n",
        "# Calculate the Residual Sum of Squares (SSR)\n",
        "ssr = sst - sse\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "# Output the results\n",
        "print(\"Total Sum of Squares (SST):\", sst)\n",
        "print(\"Explained Sum of Squares (SSE):\", sse)\n",
        "print(\"Residual Sum of Squares (SSR):\", ssr)\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"P-value:\", p_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a two-way ANOVA, you can calculate the main effects and interaction effects using Python by performing the ANOVA analysis and examining the results. You can use libraries like statsmodels or scipy to conduct the analysis and extract the main and interaction effects. Here's a step-by-step guide using statsmodels:"
      ],
      "metadata": {
        "id": "dUOugVYLOKne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Create a sample dataset (replace this with your data)\n",
        "data = {'A': [1, 2, 3, 4, 5],\n",
        "        'B': [10, 20, 30, 40, 50],\n",
        "        'Y': [15, 25, 40, 35, 50]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform two-way ANOVA\n",
        "formula = 'Y ~ A + B + A:B'  # A*B represents the interaction term\n",
        "model = ols(formula, data=df).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "# Extract main effects and interaction effect\n",
        "main_effect_A = anova_table.loc['A', 'sum_sq'] / anova_table.loc['A', 'df']\n",
        "main_effect_B = anova_table.loc['B', 'sum_sq'] / anova_table.loc['B', 'df']\n",
        "interaction_effect = anova_table.loc['A:B', 'sum_sq'] / anova_table.loc['A:B', 'df']\n",
        "\n",
        "# Output the results\n",
        "print(\"Main Effect A:\", main_effect_A)\n",
        "print(\"Main Effect B:\", main_effect_B)\n",
        "print(\"Interaction Effect (A*B):\", interaction_effect)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVZq_P9bNHkg",
        "outputId": "1d1e346c-01b9-4a07-e9fd-fcb6eab1c1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main Effect A: 640.0000000119927\n",
            "Main Effect B: 639.9999999999187\n",
            "Interaction Effect (A*B): 7.1428571428562915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "Replace the data dictionary with your actual dataset.\n",
        "Specify the formula for the ANOVA analysis using the formula variable. In this example, A + B + A:B represents the two main effects (A and B) and the interaction effect (A*B).\n",
        "Fit the ANOVA model using ols from statsmodels.\n",
        "Use sm.stats.anova_lm to generate the ANOVA table.\n",
        "Extract the main effect for A, main effect for B, and the interaction effect (A*B) from the ANOVA table.\n",
        "The main_effect_A, main_effect_B, and interaction_effect variables will contain the calculated effects. You can interpret these values to understand the contributions of each factor and their interaction to the variability in the dependent variable."
      ],
      "metadata": {
        "id": "_wg3C7pjOXus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a one-way ANOVA, the F-statistic and p-value are used to determine whether there are statistically significant differences between the group means. Here's how to interpret the results you provided:\n",
        "\n",
        "F-Statistic (5.23): The F-statistic is a measure of the variability between group means relative to the variability within the groups. A larger F-statistic suggests that there is more variation between group means compared to within-group variation. In your case, the F-statistic is 5.23.\n",
        "\n",
        "P-Value (0.02): The p-value indicates the probability of obtaining an F-statistic as extreme as the one calculated (or more extreme) under the assumption that there are no significant differences between the groups (i.e., under the null hypothesis). A smaller p-value suggests stronger evidence against the null hypothesis.\n",
        "\n",
        "Now, let's interpret these results:\n",
        "\n",
        "Null Hypothesis (H0): The null hypothesis in ANOVA is that there are no significant differences between the group means. In other words, all group means are equal.\n",
        "\n",
        "Alternative Hypothesis (Ha): The alternative hypothesis is that at least one group mean is significantly different from the others.\n",
        "\n",
        "Given the results:\n",
        "\n",
        "F-Statistic (5.23): This indicates that there is some degree of variation between the group means. However, to determine if this variation is statistically significant, we need to consider the p-value.\n",
        "\n",
        "P-Value (0.02): The p-value is less than the typical significance level (alpha) of 0.05. This suggests that the probability of obtaining the observed F-statistic under the null hypothesis (where there are no significant differences between group means) is only 0.02 (2%). Since this p-value is less than alpha, we reject the null hypothesis."
      ],
      "metadata": {
        "id": "-3u8MF6tXBkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing data in a repeated measures ANOVA is a crucial aspect of data analysis. Missing data can occur for various reasons, and how you handle it can impact the validity and interpretation of your results. Here's how to handle missing data in a repeated measures ANOVA and the potential consequences of using different methods:\n",
        "\n",
        "Listwise Deletion (Complete Case Analysis):\n",
        "\n",
        "Method: Exclude cases with any missing data from the analysis.\n",
        "Consequences: This method is straightforward but can lead to a loss of statistical power, especially if you have a large amount of missing data. It assumes that the data are missing completely at random (MCAR), which may not be the case. If data are not MCAR, this method can introduce bias.\n",
        "Pairwise Deletion (Available Case Analysis):\n",
        "\n",
        "Method: Include all cases in the analysis, but for each test, use only the cases with complete data for that specific test.\n",
        "Consequences: This method utilizes all available data but can result in different sample sizes for different tests, potentially leading to unequal variances and difficulties in interpreting results. It also assumes MCAR.\n",
        "Mean Imputation:\n",
        "\n",
        "Method: Replace missing values with the mean of the observed values for that variable.\n",
        "Consequences: Mean imputation can artificially reduce variability and bias the results, especially in repeated measures data. It assumes that the data are missing at random (MAR), which may not be valid.\n",
        "Last Observation Carried Forward (LOCF):\n",
        "\n",
        "Method: Use the most recent observed value to impute missing data.\n",
        "Consequences: LOCF may not accurately represent changes in the data, especially if there are systematic trends over time. It can underestimate variability and lead to biased results.\n",
        "Multiple Imputation:\n",
        "\n",
        "Method: Generate multiple imputed datasets with different imputed values, perform the analysis on each dataset, and combine the results to obtain unbiased estimates.\n",
        "Consequences: Multiple imputation is a robust method, but it can be computationally intensive and may require assumptions about the missing data mechanism (MAR). If the assumptions are not met, it can lead to incorrect results.\n",
        "Model-Based Imputation:\n",
        "\n",
        "Method: Fit a statistical model (e.g., mixed-effects model) to the observed data and use the model to impute missing values.\n",
        "Consequences: This approach can provide more accurate imputations if the model assumptions are met. However, model misspecification can lead to biased results.\n",
        "The choice of method should consider the nature of the data and the assumptions about the missing data mechanism. It's essential to document the method used and discuss its potential limitations in your research report or publication. In practice, sensitivity analyses and comparing results across different imputation methods can help assess the robustness of your findings."
      ],
      "metadata": {
        "id": "UPTgXpC6oWsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-hoc tests are conducted after an Analysis of Variance (ANOVA) to further explore differences between specific groups when the ANOVA indicates a significant overall effect. These tests are used to identify which groups differ from each other when there are three or more groups being compared. Common post-hoc tests include:\n",
        "\n",
        "Tukey's Honestly Significant Difference (Tukey's HSD):\n",
        "\n",
        "When to Use: Tukey's HSD is often used when you have equal sample sizes and want to make all pairwise group comparisons. It controls the familywise error rate, making it suitable when you have many comparisons to make.\n",
        "Example: Suppose you conducted an ANOVA to compare the test scores of students from five different schools. If the ANOVA indicates a significant difference, you can use Tukey's HSD to determine which specific pairs of schools have significantly different mean scores.\n",
        "Bonferroni Correction:\n",
        "\n",
        "When to Use: Bonferroni correction is a conservative approach used when you want to control the overall Type I error rate by adjusting the significance level (alpha) for multiple comparisons. It's suitable when you have few comparisons to make.\n",
        "Example: In a medical study, you're comparing the effectiveness of three different treatments. After ANOVA, you want to determine which treatment groups have significantly different outcomes. You apply Bonferroni correction to adjust the alpha level for the pairwise comparisons.\n",
        "Duncan's Multiple Range Test (MRT):\n",
        "\n",
        "When to Use: Duncan's MRT is useful when you have unequal sample sizes and you want to compare groups. It's less conservative than Bonferroni but more conservative than Tukey's HSD.\n",
        "Example: In an agricultural study, you're comparing the yields of different crop varieties. Sample sizes for each variety are not equal. Duncan's MRT can help identify which crop varieties have significantly different yields.\n",
        "Scheffé's Test:\n",
        "\n",
        "When to Use: Scheffé's test is a conservative method that can be used when you have unequal sample sizes and you want to compare groups. It's especially useful when sample sizes are small.\n",
        "Example: You're conducting a study on the effects of three different diets on weight loss. Sample sizes vary among the diet groups, and you want to determine which diets lead to significantly different weight loss. Scheffé's test can be applied for this purpose.\n",
        "Games-Howell Test:\n",
        "\n",
        "When to Use: Games-Howell is a non-parametric post-hoc test used when the assumptions of equal variances and normality are violated. It's suitable when you have unequal sample sizes and heterogeneity of variances.\n",
        "Example: In a psychological study comparing the anxiety levels of participants in four different therapy groups, you find that the assumptions of ANOVA are violated. You can use the Games-Howell test to determine which therapy groups have significantly different anxiety levels.\n",
        "Dunnett's Test:\n",
        "\n",
        "When to Use: Dunnett's test is used when you have a control group and you want to compare all other groups to the control. It's often used in experimental settings where there's a baseline or control condition.\n",
        "Example: In a drug efficacy study, you have a control group receiving a placebo and several treatment groups receiving different doses of a new medication. You can use Dunnett's test to compare each treatment group to the control group to identify significant differences in effectiveness."
      ],
      "metadata": {
        "id": "U4FOGwr3okKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct a one-way ANOVA in Python to compare the mean weight loss of three diets (A, B, and C), you can use libraries like scipy or statsmodels. Here, I'll provide an example using the scipy.stats library:\n",
        "\n",
        "First, make sure you have your weight loss data for each diet stored in separate lists or arrays. I'll assume you have your data ready in the following format:"
      ],
      "metadata": {
        "id": "-6Nf6uzWo5Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Weight loss data for each diet (replace with your actual data)\n",
        "diet_A = np.array([1.2, 1.5, 2.0, 1.8, 1.0])\n",
        "diet_B = np.array([0.8, 0.7, 0.9, 0.6, 1.2])\n",
        "diet_C = np.array([2.5, 2.8, 2.7, 2.9, 3.1])\n",
        "from scipy import stats\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
        "\n",
        "# Output the results\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "id": "ehd2bvx_OMfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cd8d4f-5bf0-451b-f76b-f4be7b95deb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Statistic: 54.64468864468863\n",
            "P-value: 9.378877817729182e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of the results:\n",
        "\n",
        "F-Statistic: The F-statistic represents the ratio of the between-group variance to the within-group variance. It measures whether the means of the three diets are significantly different. Higher F-values suggest greater differences between groups.\n",
        "\n",
        "P-value: The p-value is the probability of observing the calculated F-statistic (or more extreme values) under the null hypothesis that there are no significant differences between the groups. A smaller p-value indicates stronger evidence against the null hypothesis.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "If the p-value is less than your chosen significance level (e.g., 0.05), you would reject the null hypothesis.\n",
        "If you reject the null hypothesis, it means there are significant differences between the mean weight loss of at least one pair of diets.\n",
        "We would then typically perform post-hoc tests or pairwise comparisons to identify which specific diet(s) differ significantly from each other."
      ],
      "metadata": {
        "id": "mBxmIMSRpq0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct a two-way ANOVA in Python to assess the main effects and interaction effects between software programs (Program A, B, and C) and employee experience levels (novice vs. experienced), you can use libraries like statsmodels. Here's an example of how to do this:\n",
        "\n",
        "First, make sure you have your data organized with two categorical independent variables (software programs and experience levels) and a continuous dependent variable (task completion time). I'll assume you have your data in a format similar to this:"
      ],
      "metadata": {
        "id": "PuIsBK8_p1oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "data = pd.DataFrame({\n",
        "    'Software': ['A', 'B', 'C'] * 10,  # 3 programs, repeated 10 times\n",
        "    'Experience': ['Novice', 'Experienced'] * 45,  # 2 levels, repeated 15 times each\n",
        "    'CompletionTime': [10.2, 9.5, 11.1, 11.5, 12.0, ...]  # Task completion times\n",
        "})\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Perform two-way ANOVA\n",
        "formula = 'CompletionTime ~ Software + Experience + Software:Experience'\n",
        "model = ols(formula, data=data).fit()\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "# Output the results\n",
        "print(anova_table)\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ntDCMqIDo-Iw",
        "outputId": "3091edf0-d5f0-4922-de1c-915f2f600cf3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport pandas as pd\\n\\n# Sample data (replace with your actual data)\\ndata = pd.DataFrame({\\n    'Software': ['A', 'B', 'C'] * 10,  # 3 programs, repeated 10 times\\n    'Experience': ['Novice', 'Experienced'] * 45,  # 2 levels, repeated 15 times each\\n    'CompletionTime': [10.2, 9.5, 11.1, 11.5, 12.0, ...]  # Task completion times\\n})\\n\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\n\\n# Perform two-way ANOVA\\nformula = 'CompletionTime ~ Software + Experience + Software:Experience'\\nmodel = ols(formula, data=data).fit()\\nanova_table = sm.stats.anova_lm(model, typ=2)\\n\\n# Output the results\\nprint(anova_table)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of the results from the ANOVA table:\n",
        "\n",
        "The ANOVA table will provide F-statistics and p-values for the main effects of Software, the main effects of Experience, and the interaction effect between Software and Experience.\n",
        "\n",
        "Main Effects: If the p-value for a main effect is less than your chosen significance level (e.g., 0.05), you would conclude that there is a significant main effect of that variable on task completion time. In your case, this would indicate whether software programs or employee experience levels have a significant impact on completion time.\n",
        "\n",
        "Interaction Effect: If the p-value for the interaction effect (Software:Experience) is significant, it suggests that the combined effect of software programs and experience levels on completion time is significant. In other words, there is an interaction effect where the impact of software programs on completion time depends on the level of experience, and vice versa.\n",
        "\n",
        "In your interpretation, you would look for significant main effects and interaction effects to determine whether software programs, experience levels, or their combination significantly influence task completion time. If you have significant effects, you may also want to perform post-hoc tests or further analyses to understand which specific groups or combinations differ from each other."
      ],
      "metadata": {
        "id": "2vggWWSYqx7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are comparing only two groups (control group and experimental group), you typically don't need post-hoc tests because there are no multiple groups to compare. Post-hoc tests are used when you have more than two groups and need to determine which specific groups differ from each other after finding a significant overall difference.\n",
        "\n",
        "For a two-sample t-test comparing the control and experimental groups, you can follow these steps in Python:"
      ],
      "metadata": {
        "id": "3Q-UfzGfrXeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "control_group_scores = np.array([85, 90, 88, 78, 92])  # Scores for the control group\n",
        "experimental_group_scores = np.array([95, 87, 92, 80, 88])  # Scores for the experimental group\n",
        "\n",
        "# Perform a two-sample t-test\n",
        "t_statistic, p_value = stats.ttest_ind(control_group_scores, experimental_group_scores)\n",
        "\n",
        "# Output the results\n",
        "print(\"T-Statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN-Rrm1Bp4Za",
        "outputId": "e61cc52d-5193-477d-864a-5df4a7bbc592"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Statistic: -0.5107539184552524\n",
            "P-value: 0.6233158575786586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "If the p-value is less than your chosen significance level (e.g., 0.05), you would reject the null hypothesis.\n",
        "If you reject the null hypothesis, it means there is a significant difference in test scores between the control group (traditional teaching method) and the experimental group (new teaching method). The t-statistic indicates the size and direction of this difference.\n",
        "In this case, since you have only two groups (control and experimental), and you are interested in comparing them directly, you don't need post-hoc tests. If, in the future, you want to compare multiple groups (e.g., different variations of the new teaching method), you can consider using ANOVA followed by post-hoc tests to identify specific group differences."
      ],
      "metadata": {
        "id": "Sn1oI4s1rkdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A repeated measures ANOVA is typically used when you have repeated measurements on the same subjects or entities across different conditions or time points. In your case, where you have sales data for three stores on 30 different days, a repeated measures ANOVA may not be the most appropriate analysis. Instead, a regular one-way ANOVA or a Kruskal-Wallis test (non-parametric alternative) can be used to compare the means of three independent groups (Store A, Store B, and Store C)."
      ],
      "metadata": {
        "id": "SMdHZkhRyp3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "store_A_sales = np.array([1000, 1100, 1050, 950])  # Sales data for Store A on 30 days\n",
        "store_B_sales = np.array([1200, 1250, 1300, 1150])  # Sales data for Store B on 30 days\n",
        "store_C_sales = np.array([900, 950, 850, 920])    # Sales data for Store C on 30 days\n",
        "\n",
        "# Perform a one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
        "\n",
        "# Output the results\n",
        "print(\"F-Statistic:\", f_statistic)\n",
        "print(\"P-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgDehaz1rdHZ",
        "outputId": "c9d32af1-2741-45ab-d278-34892fad0a8d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Statistic: 31.049504950495084\n",
            "P-value: 9.134931692767435e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of the results:\n",
        "\n",
        "F-Statistic: The F-statistic measures the variability between group means relative to the variability within groups. A larger F-statistic suggests greater differences between the group means.\n",
        "\n",
        "P-Value: The p-value indicates the probability of obtaining the observed F-statistic (or more extreme values) under the null hypothesis that there are no significant differences between the stores. A smaller p-value suggests stronger evidence against the null hypothesis.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "If the p-value is less than your chosen significance level (e.g., 0.05), you would reject the null hypothesis.\n",
        "If you reject the null hypothesis, it means there is a significant difference in daily sales between at least one pair of stores. However, the ANOVA test itself does not tell you which specific stores differ.\n",
        "If you find a significant difference, and you want to determine which stores differ from each other, you can perform post-hoc tests like Tukey's Honestly Significant Difference (Tukey's HSD) or Bonferroni correction. But please note that post-hoc tests are typically used when you have more than two groups, and your data appears to involve independent observations (i.e., different days). If you had multiple measures per day for each store (e.g., hourly sales), then a repeated measures ANOVA or a mixed-effects model might be appropriate."
      ],
      "metadata": {
        "id": "n9l8wL2jy3pt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XJkBW4cy4Na"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}