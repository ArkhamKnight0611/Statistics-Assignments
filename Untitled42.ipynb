{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Probability mass function is used for discrete random variables to give the probability that the variable can take on an exact value. Probability density function is used for continuous random variables and gives the probability that the variable will lie within a specific range of values.\n",
        "\n",
        "Example : The probability mass function table for a random variable X is given as follows:\n",
        "x\t0\t1\t2\t3\t4\n",
        "P(X = x)\t0\t0.1\t0.2\t0.3\t0.4Find the value of the CDF, P(X ≤ 2).\n",
        "Solution: P(X ≤ 2), can be computed by using the pmf property P(X ∈ T) =\n",
        "∑x ϵ Tf(x).\n",
        "\n",
        "P(X ≤ 2) = P(X = 0) + P(X = 1) + P(X = 2)\n",
        "= 0 + 0.1 + 0.2\n",
        "= 0.3\n",
        "\n",
        "\n",
        "Consider an example with PDF, f(x) = x + 3, when 1 < x ≤ 3. We have to find P(2 < X < 3). Integrating x + 3 within the limits 2 and 3 gives the answer 5.5."
      ],
      "metadata": {
        "id": "umZ-t_tpfPrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cumulative Density Function (CDF) is a fundamental concept in probability and statistics. It is a mathematical function that provides information about the probability that a random variable takes on a value less than or equal to a specific value. The CDF is used for both discrete and continuous random variables and is a complementary concept to the Probability Mass Function (PMF) for discrete variables and the Probability Density Function (PDF) for continuous variables."
      ],
      "metadata": {
        "id": "u7SaG04GqbgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses of CDF:\n",
        "Probability Calculation: The primary use of the CDF is to calculate probabilities associated with a random variable. For a given value\n",
        "x,F(x) provides the cumulative probability that X is less than or equal to x.\n",
        "\n",
        "Quantile Calculation: CDFs are used to find quantiles or percentiles. For example, the 25th percentile corresponds to the value of x for which F(x)=0.25 meaning that 25% of the data falls below that value.\n",
        "\n",
        "Hypothesis Testing: CDFs play a crucial role in hypothesis testing, where one compares observed data to a known distribution's CDF to make inferences about population parameters.\n",
        "\n",
        "Survival Analysis: In survival analysis, the CDF is used to describe the probability of an event (e.g., failure, death) occurring before a given time.\n",
        "\n",
        "Given the CDF F(x) for the discrete random variable X,\n",
        "\n",
        "Find: (a) P(X = 3) (b) P(X > 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Solution:\n",
        "\n",
        "CDF of a random variable ‘X’ is a function which can be defined as,\n",
        "\n",
        "FX(x) = P(X ≤ x)(a) P(X = 3)\n",
        "\n",
        "\n",
        "To obtain the CDF of the given distribution, here we have to solve till the value is less than or equal to three. From the table, we can obtain the value\n",
        "\n",
        "\n",
        "F(3) = P(X  3) = P(X = 1) + P(X = 2) + P(X = 3)\n",
        "\n",
        "\n",
        "From the table, we can get the value of F(3) directly, which is equal to  0.67.\n",
        "\n",
        "\n",
        "(b) P(X > 2)\n",
        "\n",
        "P(X > 2) = 1 - P(X ≤ 2)\n",
        "\n",
        "P(X > 2) = 1 - F(2)\n",
        "\n",
        "P(X > 2) = 1 - 0.32P\n",
        "\n",
        "(X > 2) = 0.87\n"
      ],
      "metadata": {
        "id": "nfZbKI9xqgE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normal distribution, also known as the Gaussian distribution or the bell curve, is a fundamental probability distribution that is widely used to model various real-world phenomena. It is characterized by its bell-shaped, symmetric curve. Here are some examples of situations where the normal distribution might be used as a model:\n",
        "\n",
        "Biological Measurements:\n",
        "\n",
        "Height and weight of individuals within a population often follow a normal distribution. While there is variation, most people tend to cluster around the mean height and weight.\n",
        "IQ Scores:\n",
        "\n",
        "IQ scores in a large population are often assumed to follow a normal distribution with a mean of 100 and a standard deviation of 15.\n",
        "Measurement Errors:\n",
        "\n",
        "Errors in measurements, such as in scientific experiments or manufacturing processes, often approximate a normal distribution. This is important for uncertainty analysis and quality control.\n",
        "Financial Data:\n",
        "\n",
        "Daily returns of financial assets, such as stocks, are often modeled as normally distributed, especially over short time intervals.\n",
        "Test Scores:\n",
        "\n",
        "Scores on standardized tests like the SAT or GRE tend to follow a normal distribution, with a mean around a certain value and a known standard deviation.\n",
        "Biomedical Research:\n",
        "\n",
        "Biological measurements like blood pressure, cholesterol levels, and enzyme activity can be modeled using a normal distribution.\n",
        "Quality Control:\n",
        "\n",
        "In manufacturing, the distribution of product characteristics like length, weight, or diameter is often assumed to be normal. Deviations from the mean can indicate defects.\n",
        "Now, let's discuss how the parameters of the normal distribution relate to the shape of the distribution:\n",
        "\n",
        "Mean (μ):\n",
        "\n",
        "The mean is the central point of the distribution, and it determines the peak or center of the bell curve.\n",
        "If you increase the mean, the entire curve shifts to the right (toward higher values).\n",
        "If you decrease the mean, the entire curve shifts to the left (toward lower values).\n",
        "Standard Deviation (σ):\n",
        "\n",
        "The standard deviation controls the spread or dispersion of the distribution.\n",
        "A larger standard deviation results in a wider and flatter curve, indicating more variability in the data.\n",
        "A smaller standard deviation results in a narrower and taller curve, indicating less variability in the data."
      ],
      "metadata": {
        "id": "aNm_YpgosMg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normal distribution, also known as the Gaussian distribution or the bell curve, is of paramount importance in statistics and various fields due to its unique properties and wide applicability. Here are some key reasons why the normal distribution is important:\n",
        "\n",
        "Commonality in Nature and Data:\n",
        "\n",
        "Many natural phenomena and data sets in the real world exhibit a distribution that is approximately normal. This makes it a valuable tool for modeling and understanding a wide range of situations.\n",
        "Central Limit Theorem:\n",
        "\n",
        "The central limit theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, even if the individual variables do not follow a normal distribution themselves. This theorem underlies much of statistical inference and hypothesis testing.\n",
        "Statistical Inference:\n",
        "\n",
        "In parametric statistics, the normal distribution is often used as a reference distribution. It simplifies the calculation of confidence intervals and hypothesis tests for population parameters like the mean and variance.\n",
        "Prediction and Forecasting:\n",
        "\n",
        "In various forecasting and prediction models, the assumption of normality simplifies the analysis and leads to tractable results. This is particularly useful in fields such as finance, economics, and weather forecasting.\n",
        "Quality Control and Manufacturing:\n",
        "\n",
        "Quality control processes often assume that product characteristics, such as length, weight, or diameter, follow a normal distribution. Deviations from the mean can signal defects or quality issues.\n",
        "Biostatistics and Healthcare:\n",
        "\n",
        "In medical research and healthcare, measurements like blood pressure, cholesterol levels, and body mass index often follow a normal distribution. This helps in setting reference ranges and identifying outliers.\n",
        "Psychometrics:\n",
        "\n",
        "IQ scores, personality traits, and psychological test scores are often assumed to follow a normal distribution. This is important in psychological and educational research.\n",
        "Risk Assessment:\n",
        "\n",
        "In risk analysis and insurance, the normal distribution is used to model and understand the distribution of financial risks, such as stock price movements and insurance claims.\n",
        "Sampling and Surveys:\n",
        "\n",
        "When conducting surveys or sampling from populations, the normal distribution assumption is often made, especially when dealing with large samples. It simplifies calculations and allows for the use of standard statistical techniques.\n",
        "Data Transformation:\n",
        "\n",
        "Many statistical techniques assume that data are normally distributed. If data are not normally distributed, transformations (e.g., logarithmic or Box-Cox transformations) may be applied to make them more closely resemble a normal distribution.\n",
        "Examples of Real-Life Situations with Normal Distribution:\n",
        "\n",
        "Height of Individuals: The heights of people within a population often follow a normal distribution, with most people clustered around the mean height.\n",
        "\n",
        "Daily Stock Returns: The daily returns of many stocks on financial markets are approximately normally distributed.\n",
        "\n",
        "Exam Scores: Scores on standardized exams like the SAT or GRE tend to follow a normal distribution.\n",
        "\n",
        "Blood Pressure: Blood pressure measurements in a large population often exhibit a normal distribution.\n",
        "\n",
        "Weight of Produce: In agriculture and food processing, the weight of fruits or vegetables in a batch can be approximated by a normal distribution.\n",
        "\n",
        "Temperature Data: Daily temperature readings in a specific region over a long period of time often exhibit a normal distribution."
      ],
      "metadata": {
        "id": "5JvH-wH5sWGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Distribution:\n",
        "\n",
        "The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes, typically labeled as \"success\" (usually denoted as 1) and \"failure\" (usually denoted as 0). It is named after the Swiss mathematician Jacob Bernoulli. The Bernoulli distribution is characterized by a single parameter, usually denoted as \"p,\" which represents the probability of success.\n",
        "\n",
        "Probability Mass Function (PMF) of the Bernoulli Distribution:\n",
        "The probability mass function of the Bernoulli distribution is defined as follows:\n",
        "\n",
        "p & \\text{if } x = 1 \\text{ (success)} \\\\\n",
        "1-p & \\text{if } x = 0 \\text{ (failure)}\n",
        "\\end{cases}\n",
        "Where:\n",
        "X is the random variable representing the outcome (1 for success, 0 for failure).\n",
        "p is the probability of success.\n",
        "1−p is the probability of failure.\n",
        "\n",
        "Example of Bernoulli Distribution:\n",
        "Consider a single coin flip, where \"success\" (1) represents getting a \"heads\" result, and \"failure\" (0) represents getting a \"tails\" result. In this case, the Bernoulli distribution can be used to model the probability of getting a \"heads\" (success) on a fair coin flip, and p would be 0.5 because there is a 50% chance of success.\n",
        "\n",
        "Differences Between Bernoulli and Binomial Distributions:\n",
        "\n",
        "Number of Trials:\n",
        "\n",
        "Bernoulli Distribution: Models a single trial or experiment with two possible outcomes.\n",
        "Binomial Distribution: Models the number of successes in a fixed number of independent and identical Bernoulli trials.\n",
        "Parameters:\n",
        "\n",
        "Bernoulli Distribution: Has a single parameter p, representing the probability of success in a single trial.\n",
        "Binomial Distribution: Has two parameters: n, representing the number of trials, and p, representing the probability of success in each trial.\n",
        "\n",
        "Random Variable:\n",
        "Bernoulli Distribution: The random variable takes on values of 0 or 1, representing failure or success in a single trial.\n",
        "Binomial Distribution: The random variable represents the total number of successes (which can range from 0 to n) in n trials.\n",
        "Probability Mass Function (PMF):\n",
        "Bernoulli Distribution: The PMF has two possible values:\n",
        "p for x=1 (success) and 1−p for x=0 (failure).\n",
        "Binomial Distribution: The PMF calculates the probability of getting exactly\n",
        "k successes in\n",
        "n trials and is given by the binomial coefficient formula."
      ],
      "metadata": {
        "id": "OVpygiDvtFLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the probability that a randomly selected observation from a normally distributed dataset with a mean (μ) of 50 and a standard deviation (σ) of 10 will be greater than 60, you can use the z-score formula and standard normal distribution tables. Here are the steps:\n",
        "\n",
        "Calculate the z-score for the value 60 using the formula:\n",
        "z= X-μ/σ​\n",
        "Where:\n",
        "X is the value of interest (60 in this case).\n",
        "μ is the mean of the distribution (50).\n",
        "σ is the standard deviation of the distribution (10).\n",
        "Substituting the values:\n",
        "z= 60-50/10=1\n",
        "\n",
        "Look up the z-score (1) in a standard normal distribution table or use a calculator that provides the cumulative distribution function (CDF) for the standard normal distribution. The z-score represents the number of standard deviations above or below the mean.\n",
        "\n",
        "The CDF of the standard normal distribution at z = 1 represents the probability that a randomly selected observation from a standard normal distribution is less than 1 standard deviation above the mean.\n",
        "\n",
        "To find the probability that a randomly selected observation is greater than 60, subtract the probability from step 3 from 1 (since the total probability under the curve is 1):\n",
        "\n",
        "P(X>60)=1−P(X<60)\n",
        "\n",
        "Now, calculate it:\n",
        "\n",
        "Using a standard normal distribution table or calculator, you can find that the probability of a randomly selected observation being less than 60 (P(X < 60)) for a z-score of 1 is approximately 0.8413.\n",
        "\n",
        "So,\n",
        "P(X>60)=1−0.8413=0.1587\n",
        "\n",
        "Therefore, the probability that a randomly selected observation from the given dataset, which is assumed to be normally distributed with a mean of 50 and a standard deviation of 10, will be greater than 60 is approximately 0.1587 or 15.87%."
      ],
      "metadata": {
        "id": "1Y5V8B9S6xlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuous probability distribution is a Uniform distribution and is related to the events which are equally likely to occur. It is defined by two parameters, x and y, where x = minimum value and y = maximum value. It is generally denoted by u(x, y).\n",
        "\n"
      ],
      "metadata": {
        "id": "VmLTHrwP9itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average weight gained by a person over the winter months is uniformly distributed and ranges from 0 to 30 lbs. Find the probability of a person that he will gain between 10 and 15lbs in the winter months.\n",
        "\n",
        "\n",
        "Solution:\n",
        "\n",
        "First, find the total height of the distribution. The area under the probability distribution is always 1. Since there are 30 units starting from 0 to 30) the height is 130\n",
        ".\n",
        "\n",
        "\n",
        "Then find the width of the slice of the distribution. Do this with subtracting the biggest number b from the smallest number a and you will get\n",
        "\n",
        "\n",
        "b – a = 15 – 10 = 5.\n",
        "\n",
        "Then multiply the width in Step 2 by the height in Step 1 and you will get\n",
        "\n",
        "\n",
        "Probability = 5×1/30=5/30=1/6"
      ],
      "metadata": {
        "id": "OWn2p-AR9yfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The z-score, also known as the standard score or standardization value, is a measure of how many standard deviations a data point is away from the mean of a dataset. It's a standardized way of expressing the deviation or distance of an individual data point from the mean of a distribution. The formula for calculating the z-score for a data point\n",
        "z= X-μ/σ\n",
        "\n",
        "Here's why z-scores are important and how they are used:\n",
        "\n",
        "Standardization:\n",
        "\n",
        "Z-scores standardize data, allowing you to compare values from different distributions or datasets. By converting data into a common scale, you can make meaningful comparisons.\n",
        "Normalization:\n",
        "\n",
        "Z-scores are used to normalize data, making it easier to apply statistical techniques that assume a normal distribution. This is essential in many statistical analyses.\n",
        "Identification of Outliers:\n",
        "\n",
        "Z-scores help identify outliers. Data points with z-scores that are significantly higher or lower than a threshold (e.g., ±2 or ±3) are considered outliers.\n",
        "Probability and Percentiles:\n",
        "\n",
        "Z-scores are used to calculate probabilities associated with specific values in a normal distribution. You can find the probability of a value occurring below or above a given z-score using standard normal distribution tables.\n",
        "Hypothesis Testing:\n",
        "\n",
        "In hypothesis testing, z-scores are used to calculate test statistics, assess the significance of results, and make decisions about null hypotheses.\n",
        "Confidence Intervals:\n",
        "\n",
        "Z-scores play a crucial role in the construction of confidence intervals for population parameters like the mean and proportion. They determine the margin of error.\n",
        "Data Transformation:\n",
        "\n",
        "Z-score transformations can help transform non-normally distributed data into a more normally distributed form, which is useful for some statistical analyses.\n",
        "Comparing Data Points:\n",
        "\n",
        "Z-scores allow you to compare individual data points to the mean in the context of the dataset's variability. Positive z-scores indicate values above the mean, while negative z-scores indicate values below the mean.\n",
        "Quality Control:\n",
        "\n",
        "In quality control and process monitoring, z-scores are used to detect deviations from established norms. Data points with z-scores outside acceptable limits may indicate quality issues.\n",
        "Data Visualization:\n",
        "\n",
        "Z-scores can be used to create standardized scores that are easier to visualize and interpret in graphs and charts."
      ],
      "metadata": {
        "id": "qNLN9fae_oXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the behavior of the distribution of sample means (or other sample statistics) drawn from a population, regardless of the shape of the population's distribution. It states that, under certain conditions, as the sample size increases, the distribution of the sample means approaches a normal distribution, regardless of the original population's distribution. In other words, the CLT allows us to make statistical inferences about a population based on sample means, even if the population is not normally distributed.\n",
        "\n",
        "Key points about the Central Limit Theorem:\n",
        "\n",
        "Sample Size Requirement: The CLT applies when the sample size is sufficiently large (typically n > 30). For small sample sizes, the population should be reasonably close to a normal distribution for the CLT to work effectively.\n",
        "\n",
        "Independence: Samples must be drawn independently from the population. In the case of random sampling, this assumption is usually met.\n",
        "\n",
        "Populations with Finite Variance: While the CLT is quite robust, it assumes that the population from which samples are drawn has a finite variance. In practical terms, this means that the population does not have extremely heavy tails or extreme outliers.\n",
        "\n",
        "Significance of the Central Limit Theorem:\n",
        "\n",
        "Normal Approximation: The CLT allows us to approximate the distribution of sample means with a normal distribution, regardless of the population's underlying distribution. This is highly valuable because many statistical techniques and hypothesis tests are based on the assumption of normally distributed data.\n",
        "\n",
        "Inferential Statistics: The CLT is the foundation for many inferential statistics, including hypothesis testing and confidence interval construction. It provides the basis for making statistical inferences about population parameters from sample statistics.\n",
        "\n",
        "Large-Sample Assumption: The CLT justifies the use of statistical methods that rely on the normal distribution, even when the population itself may not be normally distributed. This simplifies statistical analysis in a wide range of applications.\n",
        "\n",
        "Quality Control: In quality control and manufacturing, the CLT is used to monitor and control processes. It helps set control limits based on sample means, ensuring that products or processes are within acceptable quality standards.\n",
        "\n",
        "Sampling: When collecting data, the CLT informs us that by increasing the sample size, we can achieve a more normal distribution of sample means. This can lead to more reliable and precise estimates of population parameters.\n",
        "\n",
        "Statistical Education: The CLT is a fundamental concept in statistics education. It helps students understand the concept of sampling variability and the importance of sample size."
      ],
      "metadata": {
        "id": "vQDnV9R9AEe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the behavior of sample means from a population. To apply the CLT effectively, certain assumptions must be met. Here are the key assumptions of the Central Limit Theorem:\n",
        "\n",
        "Independence: The samples drawn from the population must be independent of each other. In other words, the selection of one sample should not influence the selection of another. Independence is typically assumed when random sampling is performed.\n",
        "\n",
        "Random Sampling: Samples should be selected randomly from the population. This means that every individual or element in the population should have an equal chance of being included in the sample. Non-random or biased sampling can lead to incorrect conclusions.\n",
        "\n",
        "Sample Size: While the CLT is quite robust, it is most effective when the sample size (n) is sufficiently large. A common rule of thumb is that n should be greater than 30. However, smaller sample sizes can also work if the population is close to a normal distribution.\n",
        "\n",
        "Finite Variance: The population from which the samples are drawn should have a finite variance. This assumption implies that the population's distribution should not have extremely heavy tails or extreme outliers that could lead to infinite variance."
      ],
      "metadata": {
        "id": "LnZCPc12ANmr"
      }
    }
  ]
}